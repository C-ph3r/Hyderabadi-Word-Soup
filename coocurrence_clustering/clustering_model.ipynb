{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Setup and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('C:/Users/msard/OneDrive/Desktop/Data Science/Fall 2024/Text Mining/Hyderabadi-Word-Soup/data_hyderabad/10k_reviews.csv')\n",
    "restaurants_df = pd.read_csv('C:/Users/msard/OneDrive/Desktop/Data Science/Fall 2024/Text Mining/Hyderabadi-Word-Soup/data_hyderabad/105_restaurants.csv')\n",
    "\n",
    "#print(reviews_df.head())\n",
    "#print(restaurants_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuisines_word_cloud_generator(folder_path,two_towers_df,wc,vectorisation=\"bow\"):\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    for idx in range(len(two_towers_df)):\n",
    "        part_chapter_str = lambda x :\\\n",
    "            str(\"Part 1_{}_\".format(two_towers_df[\"chapter_number\"].iloc[x]))\\\n",
    "                if x < 9 else\\\n",
    "                    str(\"Part 2_{}_\".format(two_towers_df[\"chapter_number\"].iloc[x]))\n",
    "        \n",
    "        if vectorisation==\"bow\":\n",
    "            ##Bow visualization\n",
    "            chapter_bow_df = word_freq_calculator([two_towers_df[\"chapter_bow_vector\"].iloc[idx]],ttt_bow_word_list, df_output=False)\n",
    "            wc.generate_from_frequencies(chapter_bow_df) ##Use color_func in word cloud?\n",
    "            wc.to_file(os.path.join(folder_path, \"WC_\"+part_chapter_str(idx)+\"BOW.png\"))\n",
    "        else:\n",
    "            ##TFIDF visualization\n",
    "            chapter_tfidf_df = word_freq_calculator([two_towers_df[\"chapter_tfidf_vector\"].iloc[idx]],ttt_tfidf_word_list, df_output=False)\n",
    "            wc.generate_from_frequencies(chapter_tfidf_df)\n",
    "            wc.to_file(os.path.join(folder_path, \"WC_\"+part_chapter_str(idx)+\"TFIDF.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\",max_words=120, width = 220,height = 220, color_func=lambda *args, **kwargs: (0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt_word_cloud_generator(\"two_towers_chapters\",two_towers_df,wc,\"bow\")\n",
    "\n",
    "ttt_word_cloud_generator(\"two_towers_chapters\",two_towers_df,wc,\"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Named Entity Recognition for dish names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_integrator(two_towers_df, df_output=False):\n",
    "    \n",
    "    pos_dict = defaultdict(list)\n",
    "\n",
    "    for idx in range(len(two_towers_df)):\n",
    "        pos_tuples_list_chapter = two_towers_df[\"chapter_pos\"].iloc[idx]\n",
    "        \n",
    "        for pos_tuple in pos_tuples_list_chapter:\n",
    "            \n",
    "            pos_dict[pos_tuple[0].lower()].append(pos_tuple[1])\n",
    "    \n",
    "    for pos_dict_key in pos_dict.keys():\n",
    "        counter_pos_list = Counter(pos_dict[pos_dict_key])\n",
    "        pos_dict[pos_dict_key] = counter_pos_list.most_common(1)[0][0]\n",
    "\n",
    "    if df_output==False:\n",
    "        return dict(pos_dict)\n",
    "    else:\n",
    "        pos_dict_df = pd.DataFrame({\"words\":pos_dict.keys(), \"most_common_pos_tag\":pos_dict.values()})\n",
    "        return pos_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-Occurence Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt_sentences = sent_tokenizer.tokenize(two_towers_book)\n",
    "two_towers_sentences_df = pd.DataFrame({\"sentences\":ttt_sentences,\"preproc_sentences\":[pipeline_v1c.main_pipeline(sentence,\\\n",
    "                                                                                         print_output=False, lemmatized=False,\\\n",
    "                                                                                              tokenized_output=True, custom_stopwords=[])\\\n",
    "                                                                                                  for sentence in ttt_sentences]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_matrix_sentence_generator(preproc_sentences):\n",
    "\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    # Compute co-occurrences\n",
    "    for sentence in tqdm(preproc_sentences):\n",
    "        for token_1 in sentence:\n",
    "            for token_2 in sentence:\n",
    "                if token_1 != token_2:\n",
    "                    co_occurrences[token_1][token_2] += 1\n",
    "\n",
    "    #ensure that words are unique\n",
    "    unique_words = list(set([word for sentence in preproc_sentences for word in sentence]))\n",
    "\n",
    "    # Initialize the co-occurrence matrix\n",
    "    co_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
    "\n",
    "    # Populate the co-occurrence matrix\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    for word, neighbors in co_occurrences.items():\n",
    "        for neighbor, count in neighbors.items():\n",
    "            co_matrix[word_index[word]][word_index[neighbor]] = count\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "    # Return the co-occurrence matrix\n",
    "    return co_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_matrix_window_generator(preproc_sentences, window_size):\n",
    "\n",
    "    co_occurrences = defaultdict(Counter)\n",
    "\n",
    "    # Compute co-occurrences\n",
    "    for sentence in tqdm(preproc_sentences):\n",
    "        for i, word in enumerate(sentence):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    co_occurrences[word][sentence[j]] += 1\n",
    "\n",
    "    #ensure that words are unique\n",
    "    unique_words = list(set([word for sentence in preproc_sentences for word in sentence]))\n",
    "\n",
    "    # Initialize the co-occurrence matrix\n",
    "    co_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
    "\n",
    "    # Populate the co-occurrence matrix\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    for word, neighbors in co_occurrences.items():\n",
    "        for neighbor, count in neighbors.items():\n",
    "            co_matrix[word_index[word]][word_index[neighbor]] = count\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "    # Return the co-occurrence matrix\n",
    "    return co_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_network_generator(cooccurrence_matrix_df, n_highest_words, output=None):\n",
    "    \n",
    "    filtered_df = cooccurrence_matrix_df.iloc[:n_highest_words, :n_highest_words]\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes for words and set their sizes based on frequency\n",
    "    for word in filtered_df.columns:\n",
    "        graph.add_node(word, size=filtered_df[word].sum())\n",
    "\n",
    "    # Add weighted edges to the graph based on co-occurrence frequency\n",
    "    for word1 in filtered_df.columns:\n",
    "        for word2 in filtered_df.columns:\n",
    "            if word1 != word2:\n",
    "                graph.add_edge(word1, word2, weight=filtered_df.loc[word1, word2])\n",
    "\n",
    "    figure = plt.figure(figsize=(14, 12))\n",
    "\n",
    "    # Generate positions for the nodes\n",
    "    pos = nx.spring_layout(graph, k=0.5)\n",
    "\n",
    "    # Calculate edge widths based on co-occurrence frequency\n",
    "    edge_weights = [0.1 * graph[u][v]['weight'] for u, v in graph.edges()]\n",
    "\n",
    "    # Get node sizes based on the frequency of words\n",
    "    node_sizes = [data['size'] * 2 for _, data in graph.nodes(data=True)]\n",
    "\n",
    "    # Create the network graph\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color='skyblue', node_size=node_sizes)\n",
    "    nx.draw_networkx_edges(graph, pos, edge_color='gray', width=edge_weights)\n",
    "    nx.draw_networkx_labels(graph, pos, font_weight='bold', font_size=12)\n",
    "\n",
    "    plt.show() \n",
    "\n",
    "    if output==\"return\":\n",
    "        return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
